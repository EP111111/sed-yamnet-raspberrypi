{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "IISNRL_labeled_by_MobileNetV1.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/x1001000/yamnet-on-raspberrypi3/blob/main/colab_notebooks/IISNRL_labeled_by_MobileNetV1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yme0yWpiTrys"
      },
      "source": [
        "# if runtime reset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_pcU4DmpTmRU",
        "outputId": "382a64e4-6bb0-43a3-ebb2-5ac0181f26ad",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        }
      },
      "source": [
        "!git clone https://github.com/qiuqiangkong/audioset_tagging_cnn.git\n",
        "!mv audioset_tagging_cnn/* ."
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'audioset_tagging_cnn'...\n",
            "remote: Enumerating objects: 370, done.\u001b[K\n",
            "remote: Counting objects: 100% (370/370), done.\u001b[K\n",
            "remote: Compressing objects: 100% (179/179), done.\u001b[K\n",
            "remote: Total 699 (delta 206), reused 340 (delta 183), pack-reused 329\u001b[K\n",
            "Receiving objects: 100% (699/699), 3.89 MiB | 1.54 MiB/s, done.\n",
            "Resolving deltas: 100% (395/395), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HTm6J8PcIiwr",
        "outputId": "82a499d0-b9e6-4721-dc32-07f34817c33c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "CHECKPOINT_PATH=\"MobileNetV1_mAP=0.389.pth\"\n",
        "!wget https://zenodo.org/record/3987831/files/MobileNetV1_mAP%3D0.389.pth?download=1 -O $CHECKPOINT_PATH"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-10-19 11:16:15--  https://zenodo.org/record/3987831/files/MobileNetV1_mAP%3D0.389.pth?download=1\n",
            "Resolving zenodo.org (zenodo.org)... 137.138.76.77\n",
            "Connecting to zenodo.org (zenodo.org)|137.138.76.77|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 23639473 (23M) [application/octet-stream]\n",
            "Saving to: ‘MobileNetV1_mAP=0.389.pth’\n",
            "\n",
            "MobileNetV1_mAP=0.3 100%[===================>]  22.54M  6.40MB/s    in 3.5s    \n",
            "\n",
            "2020-10-19 11:16:21 (6.40 MB/s) - ‘MobileNetV1_mAP=0.389.pth’ saved [23639473/23639473]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-b3JDC7QU4DM",
        "outputId": "7287fb2d-8fcf-43ab-ee31-7fc43d08aa26",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "!pip install torchlibrosa"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting torchlibrosa\n",
            "  Downloading https://files.pythonhosted.org/packages/09/a8/663a8b3e483e910051cb6b5f60b281517d22d0bdb51686d5fc8bee460041/torchlibrosa-0.0.4-py3-none-any.whl\n",
            "Installing collected packages: torchlibrosa\n",
            "Successfully installed torchlibrosa-0.0.4\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S3LNVzKmi64Q"
      },
      "source": [
        "\n",
        "\n",
        "# if runtime restart"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bteu7pfkpt_f"
      },
      "source": [
        "#import tensorflow as tf\n",
        "#import tensorflow_hub as hub\n",
        "import numpy as np\n",
        "import csv\n",
        "#import io\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython.display import Audio\n",
        "from scipy.io import wavfile\n",
        "import librosa\n",
        "import scipy.signal"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xXI-wtuEHYum"
      },
      "source": [
        "#AudioSet Ontology\n",
        "#import requests\n",
        "#r = requests.get('https://raw.githubusercontent.com/audioset/ontology/master/ontology.json')\n",
        "#len(r.json())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LizGwWjc5w6A"
      },
      "source": [
        "def ensure_sample_rate(original_sample_rate, waveform,\n",
        "                       desired_sample_rate=16000):\n",
        "  \"\"\"Resample waveform if required.\"\"\"\n",
        "  if original_sample_rate != desired_sample_rate:\n",
        "    desired_length = int(round(float(len(waveform)) /\n",
        "                               original_sample_rate * desired_sample_rate))\n",
        "    waveform = scipy.signal.resample(waveform, desired_length)\n",
        "  return desired_sample_rate, waveform"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wo9KJb-5zuz1"
      },
      "source": [
        "# wav_file_name = 'speech_whistling2.wav'\n",
        "# wav_file_name = 'test.wav'\n",
        "def waveform(wav_file_name):\n",
        "    #sample_rate, wav_data = wavfile.read(wav_file_name, 'rb')\n",
        "    wav_data, sample_rate = librosa.load(wav_file_name)\n",
        "    # stereo to mono\n",
        "    if wav_data.ndim > 1:\n",
        "        wav_data = wav_data[:,0]\n",
        "    # normalization before resampling\n",
        "    if wav_data.dtype == np.uint8:\n",
        "        wav_data = wav_data / tf.uint8.max\n",
        "    elif wav_data.dtype == np.int16:\n",
        "        wav_data = wav_data / tf.int16.max\n",
        "    elif wav_data.dtype == np.int32:\n",
        "        wav_data = wav_data / tf.int32.max\n",
        "    elif wav_data.dtype == np.float32:\n",
        "        pass\n",
        "    else:\n",
        "        print('wav_data.dtype UNKNOWN')\n",
        "    sample_rate, wav_data = ensure_sample_rate(sample_rate, wav_data)\n",
        "\n",
        "    # Show some basic information about the audio.\n",
        "    #duration = len(wav_data)/sample_rate\n",
        "    #print(f'Sample rate: {sample_rate} Hz')\n",
        "    #print(f'Total duration: {duration:.2f}s')\n",
        "    #print(f'Size of the input: {len(wav_data)}')\n",
        "\n",
        "    # Let's listen to the wav file.\n",
        "    #Audio(wav_data, rate=sample_rate)\n",
        "\n",
        "    return wav_data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BJGP6r-At_Jc"
      },
      "source": [
        "def inference(waveform):\n",
        "    with torch.no_grad():\n",
        "        model.eval()\n",
        "        batch_output_dict = model(waveform, None)\n",
        "\n",
        "    clipwise_output = batch_output_dict['clipwise_output'].data.cpu().numpy()[0]\n",
        "    \"\"\"(classes_num,)\"\"\"\n",
        "\n",
        "    sorted_indexes = np.argsort(clipwise_output)[::-1]\n",
        "\n",
        "    return [np.array(labels)[sorted_indexes[k]] for k in range(3)]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ey0iV-eEQswK"
      },
      "source": [
        "import sys\n",
        "sys.path.append('pytorch')\n",
        "sys.path.append('utils')\n",
        "import librosa\n",
        "\n",
        "from models import *\n",
        "from pytorch_utils import move_data_to_device\n",
        "import config"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c-9_TvL_SDlL",
        "outputId": "8aea90ef-df6f-4ee6-f035-0838b2e5c87e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "source": [
        "MODEL_TYPE=\"MobileNetV1\"\n",
        "CHECKPOINT_PATH=\"MobileNetV1_mAP=0.389.pth\"\n",
        "\n",
        "sample_rate = 16000\n",
        "window_size = 1024#400\n",
        "hop_size = 320#160\n",
        "mel_bins = 64\n",
        "fmin = 50\n",
        "fmax = 14000\n",
        "model_type = MODEL_TYPE\n",
        "checkpoint_path = CHECKPOINT_PATH\n",
        "#audio_path = args.audio_path\n",
        "device = torch.device('cpu')\n",
        "\n",
        "classes_num = config.classes_num\n",
        "labels = config.labels\n",
        "\n",
        "Model = eval(model_type)\n",
        "model = Model(sample_rate=sample_rate, window_size=window_size, \n",
        "        hop_size=hop_size, mel_bins=mel_bins, fmin=fmin, fmax=fmax, \n",
        "        classes_num=classes_num)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/librosa/filters.py:284: UserWarning: Empty filters detected in mel frequency basis. Some channels will produce empty responses. Try increasing your sampling rate (and fmax) or reducing n_mels.\n",
            "  warnings.warn('Empty filters detected in mel frequency basis. '\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aHLTn9ilWGWT",
        "outputId": "3e710866-3852-4cbe-87e8-6560de85cf2d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "checkpoint = torch.load(checkpoint_path, map_location=device)\n",
        "model.load_state_dict(checkpoint['model'])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uDjD548Ql_-X",
        "outputId": "4d6ea2f7-f4d4-4c65-eb9a-c80c2937098b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZbQd6-UV5tds"
      },
      "source": [
        "import os\n",
        "from os.path import isdir\n",
        "import pandas as pd\n",
        "df = pd.DataFrame(columns=['Filename','IISNRL','YAMNET #1','YAMNET #2','YAMNET #3'])\n",
        "n = 0\n",
        "for entry in os.scandir('/content/drive/My Drive/IISNRL1700'):\n",
        "    if entry.is_dir():\n",
        "        for wav_file_name in os.scandir(entry):\n",
        "            if not wav_file_name.name.startswith('.'):\n",
        "                n += 1\n",
        "                print(n)#, wav_file_name.name)\n",
        "                Waveform = waveform(wav_file_name.path)[None,:]\n",
        "                Waveform = move_data_to_device(Waveform, None)\n",
        "                try:\n",
        "                    top1, top2, top3 = inference(Waveform)\n",
        "                except:\n",
        "                    print(wav_file_name.name)\n",
        "                    df.loc[len(df)+1] = None\n",
        "                    df.iloc[-1]['Filename'] = wav_file_name.name\n",
        "                    df.iloc[-1]['IISNRL'] = 'Error: maybe wav too short!'\n",
        "                else:\n",
        "                    df.loc[len(df)+1] = None\n",
        "                    df.iloc[-1]['Filename'] = wav_file_name.name\n",
        "                    df.iloc[-1]['IISNRL'] = wav_file_name.name.split('_')[0]\n",
        "                    df.iloc[-1]['YAMNET #1'] = top1\n",
        "                    df.iloc[-1]['YAMNET #2'] = top2\n",
        "                    df.iloc[-1]['YAMNET #3'] = top3\n",
        "                #print(df)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HqnoNZ4Tir0O"
      },
      "source": [
        "df.to_csv('/content/drive/My Drive/IISNRL1700_labeled_by_MobileNetV1.csv')"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}